{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d99b8949",
      "metadata": {
        "id": "d99b8949"
      },
      "source": [
        "# Exercise 6 - Dimensionality Reduction with PCA, Kernel PCA and FA(Factor Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "638c8b58-efde-47b9-b114-93b35d659f68",
      "metadata": {
        "id": "638c8b58-efde-47b9-b114-93b35d659f68"
      },
      "source": [
        "Our goal is to explore different tools for reducing the number of dimensions in our high-dimensional traffic flow dataset. Each technique has a unique approach to identifying and preserving the most important patterns and information within the data:\n",
        "\n",
        "PCA finds components that maximize explained variance.\n",
        "Kernel PCA extends this to discover non-linear patterns.\n",
        "FA models the data to uncover hidden latent factors that explain feature correlations.\n",
        "We will follow a common workflow for each:\n",
        "\n",
        "Standardize the data (a crucial step for PCA and Kernel PCA, and highly beneficial for FA).\n",
        "Apply the tool to project our data onto a new, lower-dimensional subspace.\n",
        "Analyze the results to understand what information is preserved and how to interpret it.\n",
        "Discuss the trade-offs and implications of using each technique."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de27be78",
      "metadata": {
        "id": "de27be78"
      },
      "source": [
        "## Data preparation\n",
        "The provided dataset is 5-minute observations on highway microwave sensors and needs to be in a format ready for dimensionality reduction. Thus, we represent days as ordered vectors of day-time observations of size 288 (there are 288, 5-minute time observations).\n",
        "\n",
        "The original dataset is in the long format (each row represents one 5 minutes interval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd69895",
      "metadata": {
        "id": "0dd69895"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#upload the dataset by downloading both datasets from canvas and upload it on colab\n",
        "data_df = pd.read_csv(\"dataset_exercise_5_clustering_highway_traffic.csv\",sep=\";\")\n",
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dcfa5e7",
      "metadata": {
        "id": "6dcfa5e7"
      },
      "source": [
        "### Long to wide transformation\n",
        "\n",
        "The following steps transform the dataset from long to wide format.\n",
        "\n",
        "The resulting format will have for each day all the 288 measured 5 minutes intervals as columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233869ac",
      "metadata": {
        "id": "233869ac"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame 'data_df' by columns \"Date\" and \"Interval_5\"\n",
        "data_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "# Extract unique dates from the sorted DataFrame\n",
        "days = np.unique(data_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days\n",
        "ndays = len(days)\n",
        "# Group the DataFrame 'data_df' by the \"Date\" column\n",
        "day_subsets_df = data_df.groupby([\"Date\"])\n",
        "# Define the total number of 5-minute intervals in a day\n",
        "nintvals = 288"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0288645a",
      "metadata": {
        "id": "0288645a"
      },
      "outputs": [],
      "source": [
        "# Create a matrix 'vectorized_day_dataset' filled with NaN values\n",
        "vectorized_day_dataset = np.zeros((ndays, nintvals))\n",
        "vectorized_day_dataset.fill(np.nan)\n",
        "# Loop through each unique day\n",
        "for i in range(0, ndays):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_subsets_df.get_group(days[i],)\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame\n",
        "        df_t = day_subsets_df.get_group(days[i],)\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset'\n",
        "        vectorized_day_dataset[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "# Print the resulting 'vectorized_day_dataset' and the size of vector\n",
        "print(vectorized_day_dataset)\n",
        "print(vectorized_day_dataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e55477",
      "metadata": {
        "id": "61e55477"
      },
      "source": [
        "### Handling missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f7f4f9",
      "metadata": {
        "id": "d4f7f4f9"
      },
      "outputs": [],
      "source": [
        "# print the number of days with missing value\n",
        "nans_per_day = np.sum(np.isnan(vectorized_day_dataset),1)\n",
        "print('number of days with missing value:',np.size(np.where(nans_per_day > 0),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e048ae3b",
      "metadata": {
        "id": "e048ae3b"
      },
      "source": [
        "Drop the days with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6059bf14",
      "metadata": {
        "id": "6059bf14"
      },
      "outputs": [],
      "source": [
        "# Drop the days with missing valus\n",
        "vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "# days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "print(vectorized_day_dataset_no_nans.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c94371a",
      "metadata": {
        "id": "3c94371a"
      },
      "source": [
        "### Task 1: Perform PCA in dataset 1 for dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29c5974",
      "metadata": {
        "id": "a29c5974"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2363c2",
      "metadata": {
        "id": "4f2363c2"
      },
      "source": [
        " #### Step 1: Overview of the Dataset\n",
        "\n",
        " Load and inspect the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b91aa8",
      "metadata": {
        "id": "39b91aa8"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(vectorized_day_dataset_no_nans)\n",
        "data.head()  # Display the first few rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cd5a64",
      "metadata": {
        "id": "78cd5a64"
      },
      "source": [
        "Step 2: Data Preprocessing (Normalization)\n",
        "\n",
        "PCA is sensitive to the scale of the features. Our features are all traffic flow counts, so they are on the same scale, but they might have different means and variances. Normalization (scaling to zero mean and unit variance) ensures that each feature contributes equally to the analysis. Without it, features with naturally larger ranges (flow during rush hour vs. nighttime) would dominate the principal components.\n",
        "\n",
        "StandardScaler transforms the data so that each feature has a mean of 0 and a standard deviation of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa131fa",
      "metadata": {
        "id": "1aa131fa"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "data_normalized = scaler.fit_transform(data)\n",
        "data_normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a96db59a",
      "metadata": {
        "id": "a96db59a"
      },
      "source": [
        "Step 3: Perform PCA for Dimensionality Reduction\n",
        "\n",
        "We will start by arbitrarily projecting our 288-dimensional data down to 6 dimensions. We will analyze the result of this choice shortly.\n",
        "\n",
        "The fit_transform method learns the transformation (finds the components that maximize variance) and then applies it to our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf8b112",
      "metadata": {
        "id": "baf8b112"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=6)  # Adjust the number of components (dimensions) as needed\n",
        "data_pca = pca.fit_transform(data_normalized)\n",
        "print(data_pca)\n",
        "print(data_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd6fa3b8",
      "metadata": {
        "id": "fd6fa3b8"
      },
      "source": [
        "Step 4: Assess Performance of PCA\n",
        "\n",
        "How good was this reduction? Did we retain most of the information? We use the explained variance ratio to answer this. This attribute tells us the percentage of the total variance in the original dataset that is captured by each principal component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7584f09",
      "metadata": {
        "id": "e7584f09"
      },
      "outputs": [],
      "source": [
        "# Investigate Explained Variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f'Explained Variance Ratio: {explained_variance}')\n",
        "print(f\"Sum of the explained variance: {sum(explained_variance)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9ac384b-4168-46fb-957e-9d5c320ca787",
      "metadata": {
        "id": "d9ac384b-4168-46fb-957e-9d5c320ca787"
      },
      "source": [
        "The first principal component explains ~31.7% of the total variance in the data. The second explains ~19.3%. Together, our first six components explain roughly 72.4% of the total variance. This means we've compressed the data into 6 dimensions while losing about 27.6% of the original information. Is this good? It depends on our goal, which we will explore next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe498f7e",
      "metadata": {
        "id": "fe498f7e"
      },
      "outputs": [],
      "source": [
        "pca.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3b4cfd",
      "metadata": {
        "id": "bc3b4cfd"
      },
      "outputs": [],
      "source": [
        "pca.components_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df371fbd",
      "metadata": {
        "id": "df371fbd"
      },
      "source": [
        "### **Task 1.1.: What is a potential issue of PCA in terms of interpretability of the results?**\n",
        "\n",
        "What do these components represent? Think about the implications that might have on the machine learning task usually following after the dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "these components represent the amount of values that affect each of the six components. In this case we would have 6 principle components that are affected by 288 values each.\n",
        "\n",
        "This dimensionality reduction would make the datasets less computational heavy. As we have less principle components. Furthermore"
      ],
      "metadata": {
        "id": "NEePOInTMLVn"
      },
      "id": "NEePOInTMLVn"
    },
    {
      "cell_type": "markdown",
      "id": "c7f2c2b6",
      "metadata": {
        "id": "c7f2c2b6"
      },
      "source": [
        "### **Task 1.2.: How many dimensions do we need to preserve at least 85% of the variance?**\n",
        "\n",
        "Our choice of n_components=6 was arbitrary. A common rule of thumb is to choose a number of components that preserves a high percentage of the variance (85% or 95%).\n",
        "\n",
        "Let's run PCA again with different amount of components"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 22)\n",
        "data_pca = pca.fit_transform(data_normalized)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f'Explained Varicance ratio:{data_pca}')\n",
        "print(f'Sum of the explained variance: {sum(explained_variance)}')"
      ],
      "metadata": {
        "id": "oYgcCbg5PFvz"
      },
      "id": "oYgcCbg5PFvz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need atleast 22 components to garner a variance percentage of 85%."
      ],
      "metadata": {
        "id": "Cddmd2PBPpl-"
      },
      "id": "Cddmd2PBPpl-"
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 84)\n",
        "data_pca = pca.fit_transform(data_normalized)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f'Explained Variance Ratio:{explained_variance}')\n",
        "print(f'Sum of the explained Variance: {sum(explained_variance)}')"
      ],
      "metadata": {
        "id": "reFOiK_BPwbM"
      },
      "id": "reFOiK_BPwbM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need atleast 84 components to get a variance percentage of 95%."
      ],
      "metadata": {
        "id": "SwcdFWedQRN5"
      },
      "id": "SwcdFWedQRN5"
    },
    {
      "cell_type": "markdown",
      "id": "ba043e84",
      "metadata": {
        "id": "ba043e84"
      },
      "source": [
        "### Step 5: Stability Analysis for PCA\n",
        "\n",
        "Stability Analysis for PCA involves assessing the variability of principal component scores across different random splits or subsets of the data. This can be done to ensure that the identified principal components are robust and not overly influenced by specific data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5afbe4b2",
      "metadata": {
        "id": "5afbe4b2"
      },
      "outputs": [],
      "source": [
        "# Conduct Stability Analysis\n",
        "num_runs = 10  # Number of times to run PCA with different random seeds\n",
        "\n",
        "prop_data_used = 0.8 # Define the proportion of data to use in each iteration (e.g., 80%)\n",
        "\n",
        "data_normalized_df = pd.DataFrame(data_normalized)\n",
        "\n",
        "for i in range(num_runs):\n",
        "    # Randomly select a subset of the data\n",
        "    prop_data_used\n",
        "    subset_indices = np.random.choice(data_normalized_df.shape[0], size=int(prop_data_used * data_normalized_df.shape[0]), replace=False)\n",
        "    subset_data = data_normalized_df.loc[subset_indices]\n",
        "    # Fit PCA on the subset\n",
        "    pca = PCA(n_components=10)  # Change random_state for each run\n",
        "    X_pca = pca.fit_transform(subset_data)\n",
        "\n",
        "    explained_variance = pca.explained_variance_ratio_\n",
        "    print(f'Run {i+1} - Explained Variance Ratio: {explained_variance}, Sum: {sum(explained_variance)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480cdad3",
      "metadata": {
        "id": "480cdad3"
      },
      "source": [
        "### **Task 1.3.: Interprete the stability analysis of the PCA**\n",
        "\n",
        "What does the result tell us?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results tell us that we get a summed explained variance around 78% to 79%\n",
        "I also not that the results differ each run, this can be explained by the fact that we take the principle components randomly each run and these does not have the same variance ratio.\n",
        "\n",
        "Furthermore I would say that the data is stable as the sum of explained variance are rather close to each other in each run."
      ],
      "metadata": {
        "id": "d-97pNJPTYfp"
      },
      "id": "d-97pNJPTYfp"
    },
    {
      "cell_type": "markdown",
      "id": "dc6b82d4",
      "metadata": {
        "id": "dc6b82d4"
      },
      "source": [
        "### Task 2: Perform PCA in dataset 1 for outlier detection\n",
        "\n",
        "PCA finds the directions of maximum variance in the data. The first few principal components are designed to capture these dominant, common patterns (the daily rhythm of traffic). The remaining components often capture much less variance, which can include noise, subtle patterns, or, importantly, anomalies.\n",
        "\n",
        "Here is the process:\n",
        "\n",
        "1. We will use PCA to project our high-dimensional data onto a lower-dimensional subspace defined by the first few principal components (we'll start with 2).\n",
        "2. We will then reverse the process, transforming this reduced data back into the original 288-dimensional space. This is called reconstruction.\n",
        "3. For each data point (each day), we will calculate the reconstruction error—the difference between the original data point and its reconstructed version.\n",
        "4. Days that are \"typical\" and follow the common patterns captured by the first 2 PCs will reconstruct well, resulting in a small error. Days that are unusual outliers (a day with a bizarre traffic pattern due to an accident, a holiday, or a sensor error) will not be well-described by the main patterns. Therefore, their reconstruction error will be very high.\n",
        "\n",
        "By setting a threshold on this error, we can automatically flag potential outliers for further investigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8f3287",
      "metadata": {
        "id": "2f8f3287"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = pd.DataFrame(vectorized_day_dataset_no_nans)\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(data)\n",
        "\n",
        "# Apply PCA for reconstruction\n",
        "pca = PCA(n_components=2)\n",
        "X_dimensionality_reduced = pca.fit_transform(X_normalized)\n",
        "X_reconstructed = pca.inverse_transform(X_dimensionality_reduced)\n",
        "\n",
        "# Calculate reconstruction errors\n",
        "reconstruction_error = np.mean(np.square(X_normalized - X_reconstructed), axis=1)\n",
        "\n",
        "# Set a threshold for the reconstruction error\n",
        "threshold = 1 # Adjust as needed\n",
        "\n",
        "# Identify outliers\n",
        "outliers = np.where(reconstruction_error > threshold)[0]\n",
        "\n",
        "# Visual Inspection of PCA Score Plots\n",
        "pca_scores = pca.fit_transform(X_normalized)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(pca_scores[:, 0], pca_scores[:, 1], c='b', alpha=0.5, label='Normal')\n",
        "plt.scatter(pca_scores[outliers, 0], pca_scores[outliers, 1], c='r', label='Outliers')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.title('PCA Score Plot')\n",
        "plt.show()\n",
        "\n",
        "# Print the indices of detected outliers\n",
        "print(f'Number of detected outliers: {outliers.shape[0]}')\n",
        "print(f'Detected outliers: {outliers}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e76e0c",
      "metadata": {
        "id": "e1e76e0c"
      },
      "source": [
        "### Using kernel PCA\n",
        "\n",
        "Linear PCA can only find flat planes/hyperplanes of maximum variance. Kernel PCA can find curved surfaces.\n",
        "\n",
        "We will now apply Kernel PCA with an RBF (Radial Basis Function) kernel to see if it can capture the structure of our data differently. The gamma parameter defines how far the influence of a single training example reaches (a key hyperparameter for RBF). Notice that there exist many other functions such as 'linear', 'poly', 'sigmoid'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee7a3c0",
      "metadata": {
        "id": "2ee7a3c0"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = pd.DataFrame(vectorized_day_dataset_no_nans)\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(data)\n",
        "\n",
        "# Step 1: Perform Kernel PCA\n",
        "# Kernel functions : 'linear', 'rbf', 'poly', 'sigmoid'.\n",
        "kpca = KernelPCA(kernel='linear', n_components=2, fit_inverse_transform=True)\n",
        "X_kpca = kpca.fit_transform(X_normalized)\n",
        "\n",
        "# Step 2: Detecting Outliers through Computing Reconstruction Errors\n",
        "# Reconstruct the data from the reduced kernel PCA space\n",
        "X_reconstructed = kpca.inverse_transform(X_kpca)\n",
        "\n",
        "# Calculate the reconstruction error\n",
        "reconstruction_error = np.mean(np.square(X_normalized - X_reconstructed), axis=1)\n",
        "\n",
        "# Define a threshold for outliers\n",
        "threshold = 1  # Adjust as needed\n",
        "kpca_scores = kpca.fit_transform(X_normalized)\n",
        "# Identify outliers\n",
        "outliers = np.where(reconstruction_error > threshold)[0]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(kpca_scores[:, 0], kpca_scores[:, 1], c='b', alpha=0.5, label='Normal')\n",
        "plt.scatter(kpca_scores[outliers, 0], kpca_scores[outliers, 1], c='r', label='Outliers')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.title('KPCA Score Plot')\n",
        "plt.show()\n",
        "# Print the number of detected outliers\n",
        "print(f'Number of detected outliers: {outliers.shape[0]}')\n",
        "print(f'Detected outliers: {outliers}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c897961d",
      "metadata": {
        "id": "c897961d"
      },
      "source": [
        "### **Task 2.1.: Why does the Kernel PCA detects the same outliers as the standard PCA when using a linear kernel?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bdb138b",
      "metadata": {
        "id": "6bdb138b"
      },
      "source": [
        "### **Task 2.2.: Do you have an idea how we could decide if we should use PCA, kernel PCA and what kernel of kernel PCA?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4d51c7",
      "metadata": {
        "id": "1f4d51c7"
      },
      "source": [
        "### Task 3: Identify the underlying latent factors that explain the observed correlations among several variables using FA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7185fa5",
      "metadata": {
        "id": "c7185fa5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "!pip install factor_analyzer\n",
        "from factor_analyzer import FactorAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "# Load and preprocess the data\n",
        "data = pd.read_csv('Dataset 2.csv')\n",
        "\n",
        "# Step 2: Exploratory Factor Analysis\n",
        "# Initialize FactorAnalyzer object\n",
        "fa = FactorAnalyzer()\n",
        "\n",
        "# # Experiment with different rotation methods (e.g., Varimax, Promax, and Oblimin)\n",
        "# # Varimax Rotation\n",
        "# fa.set_params(rotation='varimax')\n",
        "# fa.fit(data)\n",
        "\n",
        "# # Promax Rotation\n",
        "# fa.set_params(rotation='promax')\n",
        "# fa.fit(data)\n",
        "\n",
        "# # Oblimin Rotation\n",
        "fa.set_params(rotation='oblimin')\n",
        "fa.fit(data)\n",
        "\n",
        "# Step 3: Determine the Number of Factors\n",
        "# Kaiser's Criterion\n",
        "eigenvalues, _ = fa.get_eigenvalues()\n",
        "num_factors_kaiser = sum(eigenvalues > 1)  # Select factors with eigenvalues > 1\n",
        "\n",
        "# Scree Plot Examination\n",
        "plt.scatter(range(1, data.shape[1]+1), eigenvalues)\n",
        "plt.plot(range(1, data.shape[1]+1), eigenvalues)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Factor Number')\n",
        "plt.ylabel('Eigenvalue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eae4e50",
      "metadata": {
        "id": "0eae4e50"
      },
      "source": [
        "### **Task 3.1.: Identify how many factors are in the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f09985c",
      "metadata": {
        "id": "4f09985c"
      },
      "outputs": [],
      "source": [
        "# Step 4: Compute Factor Loadings\n",
        "fa.set_params(n_factors=num_factors_kaiser)  # Use the determined number of factors\n",
        "fa.fit(data)\n",
        "factor_loadings = fa.loadings_\n",
        "\n",
        "# Step 5: Interpret the Factors\n",
        "# (Based on factor loadings and variable names)\n",
        "# Provide meaningful labels/names to the identified factors based on interpretation\n",
        "factor_names = [f\"Factor {id+1}\" for id in range(0,num_factors_kaiser)]  # Adapt as needed\n",
        "\n",
        "# Print factor loadings\n",
        "factor_loadings = pd.DataFrame(factor_loadings, index=data.columns, columns=factor_names)\n",
        "print(\"Factor Loadings:\", factor_loadings)\n",
        "# print(pd.DataFrame(factor_loadings, index=data.columns, columns=factor_names))\n",
        "\n",
        "# You can further interpret and label the factors based on the loadings and variable names.\n",
        "\n",
        "# Additional: Assessing Factorability\n",
        "bartlett_test_statistic, p_value = calculate_bartlett_sphericity(data)\n",
        "kmo_score = calculate_kmo(data)\n",
        "\n",
        "print(f\"\\nBartlett's Test Statistic: {bartlett_test_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "print(f\"KMO Score: {kmo_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea27c83",
      "metadata": {
        "id": "cea27c83"
      },
      "source": [
        "### **Task 3.2. By analysing the factor loadings, specify for each factor what variables it influences**\n",
        "\n",
        "E.g.,: Factor 1: Var 1, Var 2, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "988bea92",
      "metadata": {
        "id": "988bea92"
      },
      "source": [
        "### **Task 3.3: Compare the performance of PCA and kernel PCA for dimensionality reduction in dataset 1**\n",
        "\n",
        "**Your task is to do the following:**\n",
        "\n",
        "- Experiment with different kernel functions (e.g., Gaussian, polynomial, etc.) and assess their performance for dimensionality reduction.\n",
        "- Pick the most suitable kernel function to perform kernel PCA for dimensionality reduction of dataset 1.\n",
        "- Compare the performance of kernel PCA with PCA (baseline from practice task 1) for dimensionality reduction in dataset 1 and propose a suitable method to perform dimensionality reduction in dataset 1.\n",
        "- Provide a reflection on the dimensionality reduction results obtained using PCA and kernel PCA. Was PCA sufficient to capture the data’s structure? Which one would you choose to perform your dimensionality reduction task and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd6a0c6",
      "metadata": {
        "id": "2dd6a0c6"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_df = pd.DataFrame(vectorized_day_dataset_no_nans)\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(input_df)\n",
        "\n",
        "# Step 1: Perform PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)  # Set the desired number of components\n",
        "pca_result = pca.fit_transform(data_scaled)\n",
        "\n",
        "# Step 2: Perform Kernel PCA with different kernel functions\n",
        "# kernel functions : 'linear', 'rbf', 'poly', 'sigmoid'\n",
        "\n",
        "kernel_pca_gaussian = KernelPCA(kernel='rbf', n_components=2)  # Gaussian kernel\n",
        "kernel_pca_polynomial = KernelPCA(kernel='poly', n_components=2)  # Polynomial kernel\n",
        "\n",
        "kpca_gaussian_result = kernel_pca_gaussian.fit_transform(data_scaled)\n",
        "kpca_polynomial_result = kernel_pca_polynomial.fit_transform(data_scaled)\n",
        "\n",
        "# Plot the different PCA functions\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(pca_result[:, 0], pca_result[:, 1], c='b', marker='o', alpha=0.7)\n",
        "plt.title('PCA')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(kpca_gaussian_result[:, 0], kpca_gaussian_result[:, 1], c='r', marker='o', alpha=0.7)\n",
        "plt.title('Kernel PCA (Gaussian)')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(kpca_polynomial_result[:, 0], kpca_polynomial_result[:, 1], c='b', marker='o', alpha=0.7)\n",
        "plt.title('Kernel PCA (Polynomial)')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (Spyder)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}