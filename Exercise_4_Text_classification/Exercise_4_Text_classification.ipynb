{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkcamE0EJJqx"
      },
      "source": [
        "# Preparing basic knowledge for Classification\n",
        "\n",
        "The code below is using Python with the Pandas and Scikit-learn libraries to create a word matrix from a list of strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9RYH5vPJCil"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the text\n",
        "lines = [\n",
        "    'The sun sets over the quiet meadow.',\n",
        "    'Birds chirp softly in the early morning.',\n",
        "    'A gentle breeze rustles the autumn leaves.',\n",
        "    'Stars twinkle brightly in the clear night sky.'\n",
        "]\n",
        "\n",
        "# Creat the CountVectorizer instance, the stop_words parameter is set to \"english\"\n",
        "# to remove common English stop words like \"and\", \"the\", and \"a\"\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Convert the text to vectorizer\n",
        "word_matrix = vectorizer.fit_transform(lines)\n",
        "\n",
        "# The get_feature_names_out method is used to get the feature names\n",
        "# (i.e., the individual words) in the word matrix\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# The line_names list is created to store the names of each line in the original\n",
        "# list of strings\n",
        "line_names = [f'Line {(i + 1):d}' for i, _ in enumerate(word_matrix)]\n",
        "\n",
        "# A Pandas DataFrame is created using the word_matrix.toarray() method to convert\n",
        "# the sparse matrix to a dense array\n",
        "df = pd.DataFrame(data=word_matrix.toarray(), index=line_names,columns=feature_names)\n",
        "\n",
        "# The head() method is called on the resulting DataFrame to display the first few\n",
        "# rows of the matrix\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhJtX0tfJpbH"
      },
      "source": [
        "# Exercise - Part 4.1 - Develop the text classification model for the traffic sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3x15ugDsFe"
      },
      "source": [
        "The following code is utilized for traffic sentiment analysis and employs the Pakistani Traffic Sentiment Analysis dataset as an illustrative example. This dataset comprises two columns: 'Text' and 'Sentiment.' The 'Text' column encompasses sentiments related to Pakistani traffic, encompassing both positive and negative reviews. The 'Sentiment' column is employed to categorize each sentiment, with positive reviews labeled as '0' and negative reviews as '1'.\n",
        "\n",
        "As discussed in the lecture, the sentiment analysis is modeled as a binary classification problem. The task will use the Logistic Regression as the classification model and guide you through the step-by-step process of the whole process of the model development for the traffic sentiment analysis.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML9PdR8XDsFe"
      },
      "source": [
        "Load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfQiWf5WJd5_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# The path of the dataset\n",
        "url = 'https://raw.githubusercontent.com/zhenliangma/Applied-AI-in-Transportation/master/Exercise_4_Text_classification/Pakistani%20Traffic%20sentiment%20Analysis.csv'\n",
        "\n",
        "# Load the data use the pandas\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF5ejzBqKwsC"
      },
      "source": [
        "Using the wordcloud to visualize the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBlEtHj87rcL"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "text = df['Text'].values\n",
        "\n",
        "wordcloud = WordCloud().generate(str(text))\n",
        "\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAQmaPfQvRxC"
      },
      "source": [
        "Print a brief summary of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBLWDMJOJ4l9"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIHH-6oqKI9_"
      },
      "source": [
        "Use the following statement to see how many instances there are of each class (0 for positive and 1 for negative):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVHhb1Y9KCyt"
      },
      "outputs": [],
      "source": [
        "# Displaying the instances of each class\n",
        "df.groupby('Sentiment').describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUQBRRP6KT19"
      },
      "source": [
        "There is an even number of positive and negative samples, but in each case, the number of unique samples is less than the number of samples for that class. That means the dataset has duplicate rows, and duplicate rows could bias a machine learning model. Use the following statements to delete the duplicate rows and check for balance again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVbOJxjWKNOF"
      },
      "outputs": [],
      "source": [
        "# Delete the duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Displaying the instances of each class\n",
        "df.groupby('Sentiment').describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzZoGbwfKbet"
      },
      "source": [
        "Now there are no duplicate rows, and the number of positive and negative samples is roughly equal.\n",
        "\n",
        "Next, use CountVectorizer to prepare and vectorize the text in the Text column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs4JUPApKWBF"
      },
      "outputs": [],
      "source": [
        "# Convert the text to vectorizer\n",
        "x = df['Text']\n",
        "\n",
        "# Obtain the label\n",
        "y = df['Sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87hPPRRFKq-O"
      },
      "source": [
        "Now split the dataset for training and testing. We’ll use a 80/20 split since there are almost 2,000 samples in total:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiVsNkZzKeVO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splite the train and test data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BleTI7QucYn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the CountVectorizer instance, the ngram_range represents the length range\n",
        "# of phrase segmentation,the stop_words parameter is set to \"english\" to remove\n",
        "# common English stop words like \"and\", \"the\", and \"a\", min_df is to serve as a\n",
        "# threshold to delete some words which have a frequency lower then min_df.\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english', min_df=20)\n",
        "\n",
        "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
        "x_test_vectorized = vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9psADrv9Kx50"
      },
      "source": [
        "The next step is to train a classifier. We’ll use Scikit’s LogisticRegression class, which uses logistic regression to fit a model to the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDXGS3S4KvAc"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Creat the model, max_iter controls the maximum number of iterations,\n",
        "# random_state provides a seed for the random number generator\n",
        "model = LogisticRegression(max_iter=1000, random_state=0)\n",
        "\n",
        "# Trains the logistic regression model using the training data.\n",
        "model.fit(x_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLxnBB6eK4yc"
      },
      "source": [
        "Validate the trained model with the 20% of the dataset set aside for testing and show the results in a confusion matrix and accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bQNI1UYKz30"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from sklearn.metrics import ConfusionMatrixDisplay as cmd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Show the results in a confusion matrix\n",
        "cmd.from_estimator(model, x_test_vectorized, y_test, display_labels=['Positive','Negative'], cmap='Blues')\n",
        "\n",
        "# Calculate accuracy\n",
        "print('The accuracy of the model is: ' + str(accuracy_score(y_test, model.predict(x_test_vectorized))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqabobUiLA0c"
      },
      "source": [
        "***Activity during the break: Let’s fool the model***\n",
        "\n",
        "Analyzing text for sentiment. Use the following statements to produce a sentiment score and classification for the sentence “Adayala road is clear”.\n",
        "\n",
        "you can phrase some your own reviews (e.g., suppose you want make a post on social media, what you would write about your own experience on traffic or public transport travels in your city).\n",
        "\n",
        "you can also try to fool the model and compare the results with ChatGPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP05rIBhK7_d"
      },
      "outputs": [],
      "source": [
        "# Here you change the reviews\n",
        "text = 'Adayala road is clear'\n",
        "\n",
        "# Make a prediction for this review\n",
        "score=model.predict_proba(vectorizer.transform([text]))[0][1]\n",
        "\n",
        "if score > 0.5:\n",
        "  attitude='negative'\n",
        "else:\n",
        "  attitude='positive'\n",
        "\n",
        "print('The prediction result of this review is: '+ attitude)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxzZxDEHLFis"
      },
      "source": [
        "# Exercise - Part 4.2 - Explore the performance of different text vectorization methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX8kyPqfDsFh"
      },
      "source": [
        "The task will explore the impact of text vectorization methods on traffic sentiment analysis performance. You are expected to explore three different text vectorization models: CountVectorizer, HashingVectorizer, and TfidfVectorizer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wki8RaErDsFh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import ConfusionMatrixDisplay as cmd\n",
        "import os\n",
        "\n",
        "# here you can try use different vectorization methods (Attention! uncomment the\n",
        "# method you want to test, and comment out the other methods)\n",
        "#-*-*-*-*-*-*chose different vectorization-*-*-*-*-*-*\n",
        "\n",
        "#(1) CountVectorizer\n",
        "#vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english', min_df=20)\n",
        "\n",
        "#(2) #HashingVectorizer\n",
        "#vectorizer = HashingVectorizer(ngram_range=(1, 2), n_features=200)\n",
        "\n",
        "#(3)TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "     min_df=20,\n",
        "     norm='l2',\n",
        "     smooth_idf=True,\n",
        "     use_idf=True,\n",
        "     ngram_range=(1, 2),\n",
        "     stop_words='english'\n",
        "     )\n",
        "\n",
        "#-*-*-*-*-*-*chose different vectorization-*-*-*-*-*-*\n",
        "x = df['Text']\n",
        "y = df['Sentiment']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# apply the vectorizers\n",
        "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
        "x_test_vectorized = vectorizer.transform(x_test)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, random_state=0)\n",
        "model.fit(x_train_vectorized, y_train)\n",
        "cmd.from_estimator(\n",
        "    model,\n",
        "    x_test_vectorized,\n",
        "    y_test,\n",
        "    display_labels=['Positive','Negative'],\n",
        "    cmap='Blues',\n",
        "    xticks_rotation='vertical'\n",
        "    )\n",
        "\n",
        "#calculate accuracy\n",
        "print('The accuracy of the model is: '+str(accuracy_score(y_test,model.predict(x_test_vectorized))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fts675OXDsFh"
      },
      "source": [
        "# Exercise - part 4.3-Explore the performance of different text classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avHzedDFDsFh"
      },
      "source": [
        "The task will explore the performance of different models in traffic sentiment analysis. You are expected to explore different text classification methods, including LR, KNN, RF, XGBoost, etc.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8SwgWSyDsFh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import ConfusionMatrixDisplay as cmd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "import os\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    min_df=20,\n",
        "    norm='l2',\n",
        "    smooth_idf=True,\n",
        "    use_idf=True,\n",
        "    ngram_range=(1, 1),\n",
        "    stop_words='english'\n",
        "    )\n",
        "\n",
        "# split into train/test set\n",
        "x = df['Text']\n",
        "y = df['Sentiment']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# apply the vectorizers\n",
        "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
        "x_test_vectorized = vectorizer.transform(x_test)\n",
        "\n",
        "# here you can try use different models (Attention! uncomment the method you want\n",
        "# to test, and comment out the other methods)\n",
        "\n",
        "\n",
        "#-*-*-*-*-*-*chose different models-*-*-*-*-*-*\n",
        "\n",
        "#(1)LR\n",
        "#model = LogisticRegression(max_iter=1000, random_state=0)\n",
        "\n",
        "#(2)KNN\n",
        "# model=KNeighborsClassifier()\n",
        "\n",
        "#(3)RF\n",
        "#model = RandomForestClassifier(random_state=0)\n",
        "\n",
        "#(4)XGBoost\n",
        "#model =  XGBClassifier()\n",
        "\n",
        "#(5)SVM\n",
        "# model= SVC(kernel=\"linear\")\n",
        "\n",
        "#(6)Naïve Bayes models\n",
        "model=BernoulliNB()\n",
        "#-*-*-*-*-*-*chose different models-*-*-*-*-*-*\n",
        "model.fit(x_train_vectorized, y_train)\n",
        "cmd.from_estimator(\n",
        "    model,\n",
        "    x_test_vectorized,\n",
        "    y_test,\n",
        "    display_labels=['Positive','Negative'],\n",
        "    cmap='Blues',\n",
        "    xticks_rotation='vertical'\n",
        "    )\n",
        "\n",
        "#calculate accuracy\n",
        "print('The accuracy of the model is: '+str(accuracy_score(y_test,model.predict(x_test_vectorized))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbJcT_mUDsFh"
      },
      "source": [
        "# Exercise - Assignment task-Find the best text classification model for the sentimental analysis (assignment submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jA0t3TsDsFh"
      },
      "source": [
        "The tasks for this part is use the grid search to:\n",
        "1. Identify which vectorization method works the best or basically not much difference.\n",
        "2. Identify which model, together with its corresponding hyperparameters, gives the best performance for traffic sentimental analysis.\n",
        "\n",
        "You can either use the structure below or be a be a bit more explorative and try out other strategies we have discussed in the lecture/exercises to find the best parameters/model (e.g., Random Search, ROC curve,...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KNzQrz7DsFh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import ConfusionMatrixDisplay as cmd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "import os\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "#-*-*-*-*-*-*chose different vectorization-*-*-*-*-*-*\n",
        "\n",
        "#(1) CountVectorizer\n",
        "# vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english',min_df=20)\n",
        "\n",
        "#(2) #HashingVectorizer\n",
        "# vectorizer = HashingVectorizer(ngram_range=(1, 2), n_features=200)\n",
        "\n",
        "#(3)TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "    min_df=20,\n",
        "    norm='l2',\n",
        "    smooth_idf=True,\n",
        "    use_idf=True,\n",
        "    ngram_range=(1, 1),\n",
        "    stop_words='english'\n",
        "    )\n",
        "\n",
        "#-*-*-*-*-*-*chose different vectorization-*-*-*-*-*-*\n",
        "\n",
        "# split into train/test set\n",
        "x = df['Text']\n",
        "y = df['Sentiment']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# apply the vectorizers\n",
        "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
        "x_test_vectorized = vectorizer.transform(x_test)\n",
        "\n",
        "# here you can try use the grid search to find the best model parameter(a example is in SVM model)\n",
        "#-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
        "#(1)LR\n",
        "# model = LogisticRegression(max_iter=1000, random_state=0)\n",
        "# param_grid = {\n",
        "#     'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "# }\n",
        "\n",
        "#(2)KNN\n",
        "# model=KNeighborsClassifier()\n",
        "# param_grid = {\n",
        "#     'n_neighbors': [3, 5, 7, 9],\n",
        "#     'weights': ['uniform', 'distance']\n",
        "# }\n",
        "\n",
        "#(3)RF\n",
        "# model = RandomForestClassifier(random_state=0)\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "#     'max_depth': [None, 10, 20, 30],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4]\n",
        "# }\n",
        "\n",
        "#(4)XGBoost\n",
        "# model =  XGBClassifier()\n",
        "# param_grid = {\n",
        "#     'learning_rate': [0.01, 0.1, 0.2],\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "#     'max_depth': [3, 4, 5]\n",
        "# }\n",
        "\n",
        "\n",
        "#(5)SVM\n",
        "model= SVC(probability=True)\n",
        "\n",
        "# this is an example to use the grid search to find the best parameter for SVM model\n",
        "# param_grid specifies the hyperparameter grid to search over： kernel types\n",
        "# ('linear', 'rbf', 'poly') and regularization strength C values（0.1, 1, 10）.\n",
        "param_grid = {'kernel': ['linear', 'rbf', 'poly'],'C': [0.1, 1, 10]}\n",
        "\n",
        "#`grid_search` performs a grid search with 5-fold cross-validation and evaluates models based on accuracy.\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "#`fit` method fits the model to the training data, systematically trying out all\n",
        "# parameter combinations.\n",
        "grid_search.fit(x_train_vectorized, y_train)\n",
        "\n",
        "#`best_params` and `best_score` store the best hyperparameters and their\n",
        "# corresponding accuracy score.\n",
        "best_params = grid_search.best_params_\n",
        "print(best_params)\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "#The `model` is updated with the best estimator found during the grid search,\n",
        "# which can be used for further analysis.\n",
        "model = grid_search.best_estimator_\n",
        "\n",
        "#(6)Naïve Bayes models\n",
        "# model=BernoulliNB()\n",
        "# param_grid = {'alpha': [0.1, 0.5, 1],'force_alpha': [True,False]}\n",
        "\n",
        "#-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
        "\n",
        "cmd.from_estimator(\n",
        "    model,\n",
        "    x_test_vectorized,\n",
        "    y_test,\n",
        "    display_labels=['Positive','Negative'],\n",
        "    cmap='Blues',\n",
        "    xticks_rotation='vertical'\n",
        "    )\n",
        "\n",
        "#calculate accuracy\n",
        "print('The accuracy of the model is: '+str(accuracy_score(y_test,model.predict(x_test_vectorized))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzrFFt8n4-hD"
      },
      "outputs": [],
      "source": [
        "# Here you change the reviews\n",
        "text = 'Adayala road is clear'\n",
        "\n",
        "# Make a prediction for this review\n",
        "score=model.predict_proba(vectorizer.transform([text]))[0][1]\n",
        "\n",
        "if score >0.5:\n",
        "  attitude='negative'\n",
        "else:\n",
        "  attitude='positive'\n",
        "\n",
        "print('The prediction result of this review is: '+ attitude)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2.\n",
        "\n",
        "To Simplyfy this assignment I will be using nested for loops so that I dont have to manually check every signle combination. Here the for loops will iterate through the different combinations and then we will print out the best results."
      ],
      "metadata": {
        "id": "7CLdTzIxMH0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import ConfusionMatrixDisplay as cmd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "import os\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# The path of the dataset\n",
        "url = 'https://raw.githubusercontent.com/zhenliangma/Applied-AI-in-Transportation/master/Exercise_4_Text_classification/Pakistani%20Traffic%20sentiment%20Analysis.csv'\n",
        "\n",
        "# Load the data use the pandas\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "x = df['Text']\n",
        "y = df['Sentiment']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "#Creating dicts where we store the different vectorizers so we can loop through them.\n",
        "Vectorizer = {\n",
        "    'CountVectorizer': CountVectorizer(ngram_range=(1, 2), stop_words='english',min_df=20),\n",
        "    'HashingVectorizer': HashingVectorizer(ngram_range=(1, 2), n_features=200),\n",
        "    'TfidVectorizer' : TfidfVectorizer(\n",
        "    min_df=20,\n",
        "    norm='l2',\n",
        "    smooth_idf=True,\n",
        "    use_idf=True,\n",
        "    ngram_range=(1, 1),\n",
        "    stop_words='english'\n",
        "    )\n",
        "}\n",
        "# Dict to store the models.\n",
        "models = {\n",
        "    'LR':(LogisticRegression(max_iter=1000, random_state=0),\n",
        "    {\n",
        "      'C':[0.001, 0.01, 0.1, 1, 10, 100]\n",
        "    }),\n",
        "    'KNN':(KNeighborsClassifier(),\n",
        "    {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance']\n",
        "    }),\n",
        "    'RF': (RandomForestClassifier(random_state=0),\n",
        "           {\n",
        "               'n_estimators': [100, 200],\n",
        "               'max_depth': [10, 20],\n",
        "               'min_samples_split': [2, 5],\n",
        "               'min_samples_leaf': [1, 2]\n",
        "           }),\n",
        "\n",
        "    'XGBoost': (XGBClassifier(),\n",
        "                {\n",
        "                    'learning_rate': [0.01, 0.1, 0.2],\n",
        "                    'n_estimators': [100, 200, 300],\n",
        "                    'max_depth': [3, 4, 5]\n",
        "                }),\n",
        "    'SVM': (SVC(probability=True),\n",
        "            {\n",
        "                'kernel': ['linear', 'rbf', 'poly'],\n",
        "                'C': [0.1, 1, 10]\n",
        "            }),\n",
        "    'NBM': (BernoulliNB(),\n",
        "            {\n",
        "                'alpha': [0.1, 0.5, 1],\n",
        "                'force_alpha': [True, False]\n",
        "            })\n",
        "}\n",
        "\n",
        "#This list will store the results garnered from the nested for loops of the models + vectorizers\n",
        "results = []\n",
        "\n",
        "\n",
        "for vectorizer_name, vectorizer in Vectorizer.items():\n",
        "  x_train_vectorized = vectorizer.fit_transform(x_train)\n",
        "  x_test_vectorized = vectorizer.transform(x_test)\n",
        "\n",
        "  for model_name, (model, param_grid) in models.items():#a\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy',n_jobs=-1) # im setting n_jobs = -1 to use multicore threading. To achive results faster.\n",
        "\n",
        "    grid_search.fit(x_train_vectorized, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    y_pred = best_model.predict(x_test_vectorized)\n",
        "       #Caclutate the accuracy for the combinations.\n",
        "    accuracy = accuracy_score(y_test, best_model.predict(x_test_vectorized))\n",
        "\n",
        "    results.append({\n",
        "        'Vectorizer': vectorizer_name,\n",
        "        'Model': model_name,\n",
        "        'Best Parameters': grid_search.best_params_,\n",
        "        'Grid Search Accuracy': grid_search.best_score_,\n",
        "        'Accuracy': accuracy\n",
        "          })\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "o_C_RDACMEyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmd.from_estimator(\n",
        "    best_model,\n",
        "    x_test_vectorized,\n",
        "    y_test,\n",
        "    display_labels=['Positive','Negative'],\n",
        "    cmap='Blues',\n",
        "    xticks_rotation='vertical'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "AF7DHTYKXAc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3VQUokU5stpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}