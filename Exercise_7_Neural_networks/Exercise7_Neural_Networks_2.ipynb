{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusflygar1-hash/AI_Course_Submissions_real/blob/main/Exercise_7_Neural_networks/Exercise7_Neural_Networks_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jwxWYSMMweC"
      },
      "source": [
        "# **Neural Network**\n",
        "Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.\n",
        "\n",
        "Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbJz5TA3JyBC"
      },
      "source": [
        "# **TASK 1: Develop the neural network model for predicting the count of rental bikes.â€‹**\n",
        "As experienced in regression exercise, the regression models did not perform well on the task of the count of rental bikes prediction. We look into the deep learing and This task will use the simple 2-layers fully connected neural network model as the example and guide you through the step-by-step process of the whole process of the model development.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGcXA3loalhv"
      },
      "source": [
        "## Load and prepare the data\n",
        "Start by loading the dataset and shaping it so that it's suitable for use in machine learning. This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAX-uM0-tltd",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:27:18.669676Z",
          "start_time": "2025-09-17T08:27:18.634791Z"
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sympy.printing.tensorflow import tensorflow\n",
        "\n",
        "# Define the URL from which to fetch the CSV data.\n",
        "url = 'https://raw.githubusercontent.com/zhenliangma/Applied-AI-in-Transportation/master/Exercise_7_Neural_networks/Exercise7BikeSharing.csv'\n",
        "\n",
        "# Use pandas to read the CSV data from the specified URL and store it in a DataFrame 'df'.\n",
        "#df = pd.read_csv(url)\n",
        "# Display the first 10 rows of the DataFrame 'df'.\n",
        "df = pd.read_csv(url)\n",
        "df.head(10)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0luS_xvkX_0"
      },
      "source": [
        "How many rows and columns does the dataset contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oqOWyEvqF7B",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:27:18.711248Z",
          "start_time": "2025-09-17T08:27:18.707294Z"
        }
      },
      "source": [
        "df.shape"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBtY9y0aqWnh"
      },
      "source": [
        "Are any of the columns missing values?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbnejxAwLBC",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:27:18.815561Z",
          "start_time": "2025-09-17T08:27:18.807002Z"
        }
      },
      "source": [
        "df.info()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBzn2CezpJiB"
      },
      "source": [
        "## Feature engineering\n",
        "- Target: `cnt` (count of total rental bikes including both casual and registered)\n",
        "- Predictors: weather (`temp`, `atemp`, `hum`, `windspeed`, `weathersit`), calendar (`hr`, `weekday`, `workingday`, `holiday`, `season`), and `yr`.\n",
        "- We keep it simple; you can expand features (e.g., interactions) later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjiQ_pe6pIcC",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:27:19.020978Z",
          "start_time": "2025-09-17T08:27:18.999787Z"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "target = 'cnt'\n",
        "features = [\n",
        "    'temp','atemp','hum','windspeed','weathersit',\n",
        "    'hr','weekday','workingday','holiday','season','yr'\n",
        "]\n",
        "\n",
        "X = df[features].copy()\n",
        "y = df[target].astype(float)\n",
        "\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features (important for NN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKhwhwhEkvSz"
      },
      "source": [
        "**(Optional) quick baseline (Linear Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G0qaCW4wWHv",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:27:19.076608Z",
          "start_time": "2025-09-17T08:27:19.063853Z"
        }
      },
      "source": [
        "# A super-fast baseline, just for context\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "\n",
        "rmse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print({\"LR_RMSE\": rmse_lr, \"LR_MAE\": mae_lr, \"LR_R2\": r2_lr})"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlqm-Kptw7M3"
      },
      "source": [
        "## Create a neural network model\n",
        "Now it's time build a neural network and train it with the data prepared in the previous exercise. We'll try neural network first and use cross-validation its meaasure accuracy."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-17T08:27:19.140111Z",
          "start_time": "2025-09-17T08:27:19.136663Z"
        },
        "id": "tASCjaCv6eAA"
      },
      "cell_type": "code",
      "source": [
        "#pip install tensorflow"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1nmDlwNoEiz",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:27:19.230514Z",
          "start_time": "2025-09-17T08:27:19.186536Z"
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Create a Sequential model, which is a linear stack of layers.\n",
        "model = Sequential()\n",
        "# Add a Dense layer with 32 units, ReLU activation, and an input dimension of 11.\n",
        "model.add(Dense(32, activation='relu', input_dim=11))\n",
        "# Add another Dense layer with 64 units and ReLU activation.\n",
        "model.add(Dense(64, activation='relu'))\n",
        "# Add a final Dense layer with 1 unit (typically used for regression tasks).\n",
        "model.add(Dense(1))\n",
        "# Compile the model with the Adam optimizer, Mean Absolute Error (MAE) loss function,\n",
        "# and MAE metric to be used during training.\n",
        "model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "# Display a summary of the model architecture, including the number of parameters.\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I07TdY4jtw2k"
      },
      "source": [
        "## Training and prediction\n",
        "We split the dataset into a training and a test dataset. Then, we fit the neural network model with the training dataset and predict the values for the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjB13IIZlxbQ",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:28:39.201917Z",
          "start_time": "2025-09-17T08:27:19.307498Z"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Fit the model to the training data, specifying validation split, epochs, and batch size.\n",
        "hist = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=200, batch_size=32)\n",
        "# Set the style of the plots using Seaborn.\n",
        "sns.set()\n",
        "# Extract the training and validation Mean Absolute Error (MAE) from the training history.\n",
        "err = hist.history['mae']\n",
        "val_err = hist.history['val_mae']\n",
        "# Define the number of epochs.\n",
        "epochs = range(1, len(err) + 1)\n",
        "# Plot the Training MAE and Validation MAE over epochs.\n",
        "plt.plot(epochs, err, '-', label='Training MAE')\n",
        "plt.plot(epochs, val_err, ':', label='Validation MAE')\n",
        "plt.title('Training and Validation MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.legend(loc='upper right')\n",
        "plt.plot()\n",
        "# Use the trained model to predict on the test data.\n",
        "y_pred = model.predict(X_test_scaled)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCgwHsqGvXKo"
      },
      "source": [
        "## Evaluate the model\n",
        "You can evaluate the model's performance using various metrics, such as Mean Absolute Error (MAE), Mean Squared Error (MSE), or R-squared (R2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqPg05l1vf59",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:28:39.230739Z",
          "start_time": "2025-09-17T08:28:39.223456Z"
        }
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE) between the true and predicted values.\n",
        "mae_nn = mean_absolute_error(y_test, y_pred)\n",
        "# Calculate the Root Mean Squared Error (RMSE) between the true and predicted values.\n",
        "mse_nn = mean_squared_error(y_test, y_pred)\n",
        "rmse_nn = mean_squared_error(y_test, y_pred)\n",
        "# Calculate the R-squared (R2) score, a measure of how well the model explains the variance in the data.\n",
        "r2_nn = r2_score(y_test, y_pred)\n",
        "# Print the calculated metrics.\n",
        "print(f\"Mean Absolute Error of NN: {mae_nn}\")\n",
        "print(f\"Root Mean Squared Error of NN: {rmse_nn}\")\n",
        "print(f\"R-squared of NN: {r2_nn}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW47guJtvqAW"
      },
      "source": [
        "## Compare Metrics (optional)\n",
        "we compare the performance of NN with baseline linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85GnzAS6oNyW",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:28:39.258005Z",
          "start_time": "2025-09-17T08:28:39.249898Z"
        }
      },
      "source": [
        "import pandas as pd\n",
        "results = pd.DataFrame([\n",
        "    {'Model': 'Linear Regression', 'RMSE': rmse_lr, 'MAE': mae_lr, 'R2': r2_lr},\n",
        "    {'Model': 'Neural Net', 'RMSE': rmse_nn, 'MAE': mae_nn, 'R2': r2_nn},\n",
        "])\n",
        "results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-17T08:28:39.389206Z",
          "start_time": "2025-09-17T08:28:39.301174Z"
        },
        "id": "kgfZ-8ce6eAD"
      },
      "cell_type": "code",
      "source": [
        "# Bar chart of RMSE\n",
        "plt.figure()\n",
        "plt.bar(results['Model'], results['RMSE'])\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('RMSE Comparison')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2s8moatbDE0"
      },
      "source": [
        "# **Task 2: Practical NN training techniques**\n",
        "The task will illustrate some useful NN training techniques, including saving and loading the trained model, using the callback function to save the best model, and adding dropout layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypA1w6u0Xm9m",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:28:51.983916Z",
          "start_time": "2025-09-17T08:28:39.672514Z"
        }
      },
      "source": [
        "from json import load\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "#-------------------------------------------------data preprocessing------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#-------------------------------------------------network construction-----------------------------------------------------------------\n",
        "\n",
        "#************************************************Here you can choose to add the dropout layer or not***************************\n",
        "#\n",
        "# def construct_network_model():\n",
        "#   # Create a Sequential model, which is a linear stack of layers.\n",
        "#   model = Sequential()\n",
        "#\n",
        "#   # Add a Dense layer with 32 units, ReLU activation, and an input dimension of 4.\n",
        "#   model.add(Dense(32, activation='relu', input_dim=4))\n",
        "#\n",
        "#   # # Add a Dropout layer with a dropout rate of 0.5.\n",
        "#   # model.add(Dropout(0.5))\n",
        "#\n",
        "#   # Add another Dense layer with 64 units and ReLU activation.\n",
        "#   model.add(Dense(64, activation='relu'))\n",
        "#\n",
        "#   # # Add another Dropout layer with a dropout rate of 0.5.\n",
        "#   # model.add(Dropout(0.5))\n",
        "#\n",
        "#   # Add a final Dense layer with 1 unit (typically used for regression tasks).\n",
        "#   model.add(Dense(1))\n",
        "#   return model\n",
        "#\n",
        "# model=construct_network_model()\n",
        "# # Compile the model with the Adam optimizer, Mean Absolute Error (MAE) loss function,\n",
        "# # and MAE metric to be used during training.\n",
        "# model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "#************************************************Here you can choose to add the dropout layer or not*****************\n",
        "\n",
        "#-------------------------------------------------model train------------------------------------------------------------------\n",
        "# use the callback function to early stop, learning rate ajusting, save the best model\n",
        "# Create an EarlyStopping callback to monitor the validation mean absolute error (val_mae).\n",
        "# It will stop training if val_mae doesn't improve for 5 consecutive epochs and restores the best weights.\n",
        "early_stop = EarlyStopping(monitor='val_mae', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Create a ReduceLROnPlateau callback to monitor val_mae.\n",
        "# It reduces the learning rate by a factor of 0.5 if val_mae doesn't improve for 3 consecutive epochs.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.5, patience=3)\n",
        "\n",
        "# Define the file path where the best model will be saved.\n",
        "filepath = \"weights.best.keras\"\n",
        "\n",
        "# Create a ModelCheckpoint callback to monitor the validation mae (val_mae).\n",
        "# The callback will save thhe model's weights only if the validation mae improves.\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# Train the model using the fit method.\n",
        "hist_2 = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=200, batch_size=32, callbacks=[early_stop, reduce_lr,checkpoint],verbose=0)\n",
        "\n",
        "\n",
        "#-------------------------------------------------model evaluation--------------------------------------------------------------------\n",
        "\n",
        "# Set the style of the plots using Seaborn.\n",
        "sns.set()\n",
        "\n",
        "# Extract the training and validation Mean Absolute Error (MAE) from the training history.\n",
        "err = hist_2.history['mae']\n",
        "val_err = hist_2.history['val_mae']\n",
        "\n",
        "# Define the number of epochs.\n",
        "epochs = range(1, len(err) + 1)\n",
        "\n",
        "# Plot the Training MAE and Validation MAE over epochs.\n",
        "plt.plot(epochs, err, '-', label='Training MAE')\n",
        "plt.plot(epochs, val_err, ':', label='Validation MAE')\n",
        "plt.title('Training and Validation MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.legend(loc='upper right')\n",
        "plt.plot()\n",
        "\n",
        "# Use the trained model to predict on the test data.\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R2) for the predictions.\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print('--------------------------------------'+'this is result of the trained model'+\"---------------------------------------------\")\n",
        "# Print the calculated metrics.\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")\n",
        "\n",
        "#-------------------------------------------------load model and evaluation--------------------------------------------------------------------\n",
        "if not filepath ==\"\":\n",
        "  # Load a pre-trained model from the specified file path.\n",
        "  model = load_model(filepath)\n",
        "\n",
        "  # Use the loaded model to predict on the test data.\n",
        "  y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "  # Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R2) for the predictions.\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "  print('--------------------------------------'+'this is result of the model loaded from the local path'+\"---------------------------------------------\")\n",
        "\n",
        "  # Print the calculated metrics.\n",
        "  print(f\"Mean Absolute Error: {mae}\")\n",
        "  print(f\"Mean Squared Error: {mse}\")\n",
        "  print(f\"R-squared: {r2}\")\n",
        "\n",
        "\n",
        "\n",
        "  # Create a scatter plot to visualize the relationship\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.scatter(y_test, y_pred, alpha=0.5)  # Plot actual vs. predicted values\n",
        "\n",
        "  # Add labels and title\n",
        "  plt.xlabel(\"Actual Values\")\n",
        "  plt.ylabel(\"Predicted Values\")\n",
        "  plt.title(\"Actual vs. Predicted Values\")\n",
        "\n",
        "  # Add a diagonal line for reference (perfect predictions)\n",
        "  plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', lw=2)\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3: Compare the results with a linear regression model**\n",
        "The task involves comparing the results obtained from the neural network with those from the linear regression model, and we should analyze the reasons behind any differences in the outcomes."
      ],
      "metadata": {
        "id": "Jr1eM98PPpeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Set the style of the plots using seaborn.\n",
        "sns.set()\n",
        "\n",
        "# Use the trained model to predict on the test data.\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R2) for the predictions.\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred)\n",
        "# rmse = mean_squared_error(y_test, y_pred, squared = error) This is  a bug, does not work.\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print('--------------------------------------'+'this is result of the neural network model'+\"---------------------------------------------\")\n",
        "# Print the evaluation metrics.\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "print(f\"R-squared: {r2}\")\n",
        "\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model_linear = LinearRegression()\n",
        "model_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = model_linear.predict(X_test_scaled)\n",
        "\n",
        "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
        "rmse_linear = mean_squared_error(y_test, y_pred_linear)\n",
        "# rmse_linear = mean_squared_error(y_test, y_pred_linear, squared = False), this is also a bug. does not work. There is no feature for rmse that is squared.\n",
        "r2_linear = r2_score(y_test, y_pred_linear)\n",
        "\n",
        "print('--------------------------------------'+'this is result of the linear regression model'+\"---------------------------------------------\")\n",
        "print(f\"Mean Absolute Error of linear model: {mae_linear}\")\n",
        "print(f\"Root Mean Squared Error of linear model: {rmse_linear}\")\n",
        "print(f\"R-squared of linear model: {r2_linear}\")"
      ],
      "metadata": {
        "id": "okMlrWbJSCew",
        "ExecuteTime": {
          "end_time": "2025-09-17T08:28:52.129028Z",
          "start_time": "2025-09-17T08:28:51.983916Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0voSfClhQ48i"
      },
      "source": [
        "# **Assignment task: Find the best neural network model for the count of rental bikes prediction (assignment submission)**\n",
        "Tuning the neural network models (e.g., dropout, sizing of the network), and finding the best neural network model. Suggestions:\n",
        "- Try adding interaction features (e.g., `hr * workingday`, `weathersit * hum`).\n",
        "- Tune NN hyperparameters (layers, neurons, learning rate) via `GridSearchCV`.\n",
        "- Consider more advanced architectures (e.g., gradient boosting, LSTMs for temporal structure)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/zhenliangma/Applied-AI-in-Transportation/master/Exercise_7_Neural_networks/Exercise7BikeSharing.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "8MmyYeNst-uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#Clean and set up the data set\n",
        "features = [\n",
        "    'temp','atemp','hum','windspeed','weathersit',\n",
        "    'hr','weekday','workingday','holiday','season','yr'\n",
        "]\n",
        "\n",
        "target = ['cnt']\n",
        "\n",
        "x = df[features]\n",
        "\n",
        "y = df[target]\n",
        "\n",
        "#train test split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Scaling the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "VqrkjgEgx0EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries needed to perform the NN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "#build the nn and add dense layers\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_dim=len(features))) #increased the width from 34 and 68.\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256 , activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YnUCKYM-zEEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#callback safe guards to save computational \"power\", so we dont have to iterate through all 200 epochs if we do not improve.\n",
        "callback = EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
        "# reduce lr on plateau will start reducing the learning rate if val_mae stops improving by atleast 0.5 each iteration.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.5, patience=10, min__lr = 1e-5)\n",
        "checkpoint = ModelCheckpoint(\"weights.best.keras\", monitor='val_mae', verbose=1, save_best_only=True, mode='min')"
      ],
      "metadata": {
        "id": "7S5NHDyl0arW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "1pLONaGugYVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# hist = model.fit(X_train,y_train, epochs = 200, batch_size = 32 , validation_split = 0.2, callbacks = [callback])\n",
        "# hist = model.fit(X_train,y_train, epochs = 200, batch_size = 32 , validation_split = 0.2, callbacks = [reduce_lr, checkpoint])\n",
        "hist = model.fit(X_train,y_train, epochs = 200, batch_size = 32 , validation_split = 0.2, callbacks = [callback,reduce_lr, checkpoint])\n",
        "\n",
        "#prediction.\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "sns.set()\n",
        "\n",
        "err = hist.history['mae']\n",
        "val_err = hist.history['val_mae']\n",
        "\n",
        "epochs = range(1, len(err) + 1)\n",
        "#plot so we can visualize results etc..\n",
        "#this code is taken from the earlier part of the exercise.\n",
        "plt.plot(epochs, err, '-', label='Training MAE')\n",
        "plt.plot(epochs, val_err, ':', label='Validation MAE')\n",
        "plt.title('Training and Validation MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "dIdvkyEmzMuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "#loading model\n",
        "best_model = load_model(\"weights.best.keras\")\n",
        "y_pred = best_model.predict(X_test)\n",
        "#Calculating the evaluation metrics.\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "#make a dataframe contianing the results of the nn model.\n",
        "results = pd.DataFrame([\n",
        "    {'Model': 'Neural Network', 'MAE': mae,'MSE': mse, 'R2': r2}])"
      ],
      "metadata": {
        "id": "-K87JtNf4ygW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best so far 32.8xx"
      ],
      "metadata": {
        "id": "JM7In571Eknp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "4kKG1_DBEZvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xZ0ky0VhRqU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nbJz5TA3JyBC",
        "oGcXA3loalhv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}